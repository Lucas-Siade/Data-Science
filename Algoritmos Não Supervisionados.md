# ğŸ•µï¸ Algoritmos NÃ£o Supervisionados

Os algoritmos nÃ£o supervisionados sÃ£o utilizados quando os dados de entrada nÃ£o possuem rÃ³tulos. O principal objetivo desse algoritmo Ã© identificar automaticamente padrÃµes ou relacionamentos ocultos, revelando insights que poderiam passar despercebidos em uma anÃ¡lise tradicional.

## ğŸ§² Affinity Propagation

### âš™ï¸ Principais CaracterÃ­sticas
O **Affinity Propagation** Ã© um algoritmo de agrupamento que identifica automaticamente o nÃºmero de clusters com base nas similaridades entre os dados. Ele funciona trocando mensagens entre pares de amostras, ajustando iterativamente quais pontos devem ser os representantes de cada cluster.

### âœ… Vantagens
* **Clusters Determinados Automaticamente:** NÃ£o requer que o usuÃ¡rio defina previamente o nÃºmero de grupos.

* **Capta Estruturas Complexas:** Funciona bem com clusters de formas variadas.

* **Identifica Exemplares Representativos:** Escolhe pontos reais como representantes de cada grupo.

### âŒ Desvantagens
* **Alto Custo Computacional:** Pode ser lento para grandes volumes de dados.

* **Dependente da Similaridade:** Resultados podem variar dependendo da funÃ§Ã£o de similaridade escolhida.

* **Pouco EscalÃ¡vel:** NÃ£o recomendado para datasets muito grandes.

### ğŸ¯ Principais AplicaÃ§Ãµes
* **Agrupamento de Imagens e Documentos**
* **SegmentaÃ§Ã£o de Dados**
* **Reconhecimento de PadrÃµes**

<br>

## ğŸ“¦ DBSCAN

### âš™ï¸ Principais CaracterÃ­sticas
O **DBSCAN** Ã© um algoritmo de agrupamento baseado em densidade que forma clusters a partir de regiÃµes de alta densidade de pontos. O algoritmo define como â€œnÃºcleoâ€ um ponto que possui um nÃºmero mÃ­nimo de vizinhos dentro de uma determinada distÃ¢ncia. A partir desses nÃºcleos, os clusters sÃ£o formados por pontos densamente conectados, enquanto os pontos que nÃ£o atendem aos critÃ©rios sÃ£o classificados como ruÃ­do.

### âœ… Vantagens
* **NÃ£o Requer NÃºmero de Clusters:** Define os grupos com base na densidade dos dados.

* **IdentificaÃ§Ã£o AutomÃ¡tica de Outliers:** Distingue naturalmente os ruÃ­dos.

* **DetecÃ§Ã£o de Clusters de Forma ArbitrÃ¡ria:** Consegue identificar clusters com formas complexas.

### âŒ Desvantagens
* **SensÃ­vel Ã  escolha dos parÃ¢metros:** A performance do algoritmo depende fortemente de uma boa escolha do *eps* e *min_samples*.

* **Desempenho Ruim em Densidade VariÃ¡vel:** NÃ£o lida bem com clusters com diferentes densidades.

* **Baixo Desempenho em Alta Dimensionalidade:** Em espaÃ§os de muitas dimensÃµes, a noÃ§Ã£o de "distÃ¢ncia" perde significado, o que reduz a eficÃ¡cia do algoritmo.

### ğŸ¯ Principais AplicaÃ§Ãµes
* **DetecÃ§Ã£o de Anomalias**
* **AnÃ¡lise Geoespacial**
* **SegmentaÃ§Ã£o de Clientes**

<br>

## ğŸŒ³ Hierarchical Clustering

### âš™ï¸ Principais CaracterÃ­sticas
O **Hierarchical Clustering** Ã© um algoritmo de aprendizado que agrupa dados formando uma estrutura em Ã¡rvore (dendrograma), revelando relaÃ§Ãµes hierÃ¡rquicas entre os elementos. Essa tÃ©cnica permite identificar padrÃµes e conexÃµes em conjuntos de dados, sendo Ãºtil para anÃ¡lises exploratÃ³rias e visualizaÃ§Ã£o de relaÃ§Ãµes entre clusters.

### âœ… Vantagens
* **VisualizaÃ§Ã£o com Dendrograma:** FÃ¡cil de entender a estrutura dos dados.
  
* **NÃ£o precisa definir o NÃºmero de Clusters:** O nÃºmero ideal pode ser escolhido apÃ³s analisar o dendrograma.

* **Detecta Subgrupos Naturais:** Ãštil para dados com mÃºltiplas camadas de agrupamento.

### âŒ Desvantagens
* **Pouco EscalÃ¡vel:** Alto custo computacional para grandes conjuntos de dados.

* **NÃ£o se ajusta dinamicamente:** NÃ£o tem flexibilidade para corrigir junÃ§Ãµes ou divisÃµes anteriores.

* **SensÃ­vel a RuÃ­dos e Outliers:** NÃ£o possui mecanismos robustos para lidar com anomalias ao construir a hierarquia.

### ğŸ¯ Principais AplicaÃ§Ãµes
* **ReduÃ§Ã£o de Dimensionalidade e PrÃ©-processamento**
* **Agrupamento de Documentos**
* **Taxonomia BiolÃ³gica**

<br>

## ğŸ“ K-Means

### âš™ï¸ Principais CaracterÃ­sticas
O **K-Means** Ã© um algoritmo de agrupamento que divide os dados em K grupos com base na similaridade, sendo essa similaridade geralmente medida pela distÃ¢ncia euclidiana. Cada grupo Ã© representado por um **Centroide**, que corresponde Ã  mÃ©dia dos pontos pertencentes ao respectivo grupo. O algoritmo ajusta iterativamente os centroides atÃ© que eles se estabilizem, buscando minimizar a distÃ¢ncia entre os pontos e seus centroides.

### âœ… Vantagens
* **Simples de Implementar:** O modelo Ã© simples e pode ser facilmente descrito com parÃ¢metros matemÃ¡ticos.

* **EscalÃ¡vel para grandes conjuntos de dados:** Especialmente quando o nÃºmero de *clusters* Ã© pequeno.

* **FÃ¡cil de Interpretar:** Cada ponto Ã© atribuÃ­do ao grupo cujo centrÃ³ide estÃ¡ mais prÃ³ximo.

* **RÃ¡pido para dados de alta dimensionalidade:** Utiliza cÃ¡lculos vetoriais simples.

### âŒ Desvantagens
* **O nÃºmero de *clusters* K deve ser definido previamente:** Ã‰ necessÃ¡rio determinar o valor de K antes da execuÃ§Ã£o do algoritmo, o que pode ser desafiador sem conhecimento prÃ©vio sobre a estrutura dos dados.

* **SensÃ­vel a Outliers:** Pontos muito distantes podem distorcer a posiÃ§Ã£o dos centroides.

* **Pode convergir para mÃ­nimos locais:** A escolha aleatÃ³ria dos centroides iniciais pode encontrar soluÃ§Ãµes que nÃ£o sÃ£o ideais.

### ğŸ¯ Principais AplicaÃ§Ãµes
* **Agrupamento de Produtos**
* **DetecÃ§Ã£o de Anomalias**
* **AnÃ¡lise Geoespacial**

<br>

## ğŸ“‰ PCA (Principal Component Analysis)

### âš™ï¸ Principais CaracterÃ­sticas
O **PCA** Ã© um algoritmo de **reduÃ§Ã£o de dimensionalidade**. Ele transforma o conjunto de dados original em um novo sistema de coordenadas, mantendo a maior variabilidade dos dados com o menor nÃºmero de variÃ¡veis possÃ­vel (componentes principais).

### âœ… Vantagens
* **ReduÃ§Ã£o de RuÃ­do:** Ao eliminar componentes menos relevantes, melhora o desempenho de outros modelos.

* **Facilidade de VisualizaÃ§Ã£o:** Facilita a anÃ¡lise visual de datasets multidimensionais.

* **Melhora o Desempenho Computacional:** Reduz a dimensionalidade mantendo a maior parte da informaÃ§Ã£o.

### âŒ Desvantagens
* **Perda de Interpretabilidade:** As novas variÃ¡veis (componentes principais) nÃ£o tÃªm significado direto.

* **Assume Linearidade:** NÃ£o captura bem relaÃ§Ãµes nÃ£o lineares.

* **Pode eliminar InformaÃ§Ãµes Importantes:** Se os componentes principais nÃ£o representarem bem a variabilidade dos dados.

### ğŸ¯ Principais AplicaÃ§Ãµes
* **PrÃ©-processamento de Dados para Machine Learning**
* **VisualizaÃ§Ã£o de Dados Complexos**
* **CompressÃ£o de Dados**

<br>

## ğŸ”— ReferÃªncias

### ğŸ“š Conceitos Gerais de Aprendizado NÃ£o Supervisionado
* [Oracle - Aprendizado NÃ£o Supervisionado](https://www.oracle.com/br/artificial-intelligence/machine-learning/unsupervised-learning/)
* [Deep Learning Book](https://www.deeplearningbook.com.br/)
* [Kaggle â€“ Iris Dataset](https://www.kaggle.com/datasets/uciml/iris)

---

### ğŸ§² Affinity Propagation
* [Scikit-Learn â€“ Affinity Propagation](https://scikit-learn.org/stable/modules/clustering.html#affinity-propagation)
* [Wikipedia â€“ Affinity Propagation](https://en.wikipedia.org/wiki/Affinity_propagation)

---

### ğŸ“¦ DBSCAN
* [Scikit-Learn â€“ DBSCAN](https://scikit-learn.org/stable/modules/clustering.html#dbscan)
* [Medium â€“ Entendendo DBSCAN](https://gabriellm.medium.com/entendendo-dbscan-770f680d9160)

---

### ğŸŒ³ Hierarchical Clustering
* [Scikit-Learn â€“ Hierarchical Clustering](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering)
* [IBM â€“ Hierarchical Clustering](https://www.ibm.com/think/topics/hierarchical-clustering)

---

### ğŸ“ K-Means
* [Scikit-Learn â€“ K-Means](https://scikit-learn.org/stable/modules/clustering.html#k-means)
* [IBM â€“ K-Means Clustering](https://www.ibm.com/br-pt/think/topics/k-means-clustering)

---
### ğŸ“‰ PCA (Principal Component Analysis)
* [Scikit-Learn â€“ Principal Component Analysis (PCA)](https://scikit-learn.org/stable/modules/decomposition.html#principal-component-analysis-pca)
* [IBM â€“ Principal Component Analysis](https://www.ibm.com/think/topics/principal-component-analysis)
