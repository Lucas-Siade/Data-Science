# ğŸ•µï¸ Algoritmos NÃ£o Supervisionados

Os algoritmos nÃ£o supervisionados sÃ£o utilizados quando os dados de entrada nÃ£o possuem rÃ³tulos. O principal objetivo desse algoritmo Ã© identificar automaticamente padrÃµes ou relacionamentos ocultos, revelando insights que poderiam passar despercebidos em uma anÃ¡lise tradicional.

## ğŸ§² Affinity Propagation

### âš™ï¸ Principais CaracterÃ­sticas
O **Affinity Propagation** Ã© um algoritmo de agrupamento que identifica automaticamente o nÃºmero de clusters com base nas similaridades entre os dados. Ele funciona trocando mensagens entre pares de amostras, ajustando iterativamente quais pontos devem ser os representantes de cada cluster.

### âœ… Vantagens
* **Clusters Determinados Automaticamente:** NÃ£o requer que o usuÃ¡rio defina previamente o nÃºmero de grupos.

* **Capta Estruturas Complexas:** Funciona bem com clusters de formas variadas.

* **Identifica Exemplares Representativos:** Escolhe pontos reais como representantes de cada grupo.

### âŒ Desvantagens
* **Alto Custo Computacional:** Pode ser lento para grandes volumes de dados.

* **Dependente da Similaridade:** Resultados podem variar dependendo da funÃ§Ã£o de similaridade escolhida.

* **Pouco EscalÃ¡vel:** NÃ£o recomendado para datasets muito grandes.

### ğŸ¯ Principais AplicaÃ§Ãµes
* **Agrupamento de Imagens e Documentos**
* **SegmentaÃ§Ã£o de Dados**
* **Reconhecimento de PadrÃµes**

<br>

## ğŸ“¦ DBSCAN

### âš™ï¸ Principais CaracterÃ­sticas
O **DBSCAN** Ã© um algoritmo de agrupamento baseado em densidade que forma clusters a partir de regiÃµes de alta densidade de pontos. O algoritmo define como â€œnÃºcleoâ€ um ponto que possui um nÃºmero mÃ­nimo de vizinhos dentro de uma determinada distÃ¢ncia. A partir desses nÃºcleos, os clusters sÃ£o formados por pontos densamente conectados, enquanto os pontos que nÃ£o atendem aos critÃ©rios sÃ£o classificados como ruÃ­do.

### âœ… Vantagens
* **NÃ£o Requer NÃºmero de Clusters:** Define os grupos com base na densidade dos dados.

* **IdentificaÃ§Ã£o AutomÃ¡tica de Outliers:** Distingue naturalmente os ruÃ­dos.

* **DetecÃ§Ã£o de Clusters de Forma ArbitrÃ¡ria:** Consegue identificar clusters com formas complexas.

### âŒ Desvantagens
* **SensÃ­vel Ã  escolha dos parÃ¢metros:** A performance do algoritmo depende fortemente de uma boa escolha do *eps* e *min_samples*.

* **Desempenho Ruim em Densidade VariÃ¡vel:** NÃ£o lida bem com clusters com diferentes densidades.

* **Baixo Desempenho em Alta Dimensionalidade:** Em espaÃ§os de muitas dimensÃµes, a noÃ§Ã£o de "distÃ¢ncia" perde significado, o que reduz a eficÃ¡cia do algoritmo.

### ğŸ¯ Principais AplicaÃ§Ãµes
* **DetecÃ§Ã£o de Anomalias**
* **AnÃ¡lise Geoespacial**
* **SegmentaÃ§Ã£o de Clientes**

<br>

## ğŸ“Š Gaussian Mixture Model

### âš™ï¸ Principais CaracterÃ­sticas
O **Gaussian Mixture Model** Ã© um algoritmo de agrupamento baseado em modelos probabilÃ­sticos. Ele modela a distribuiÃ§Ã£o dos pontos como uma combinaÃ§Ã£o de mÃºltiplas distribuiÃ§Ãµes gaussianas, onde cada uma representa um possÃ­vel grupo, e calcula a probabilidade de cada ponto pertencer a cada grupo.

### âœ… Vantagens
* **AtribuiÃ§Ã£o Suave:** Em vez de designar cada ponto a um Ãºnico cluster, fornece a probabilidade de pertencimento a todos os clusters.
  
* **Capta Estruturas Complexas:** Capaz de identificar clusters com diferentes formas, tamanhos e orientaÃ§Ãµes.

* **Base EstatÃ­stica Robusta:** Permite anÃ¡lise mais rica e interpretÃ¡vel com base em distribuiÃ§Ãµes gaussianas.

### âŒ Desvantagens
* **O nÃºmero de *clusters* deve ser definido previamente:** Requer a escolha prÃ©via do nÃºmero de componentes (clusters).

* **SensÃ­vel a Outliers:** Pode ser afetado por pontos fora do padrÃ£o, pois tenta ajustar gaussianas a todos os dados.

* **Mais Complexo Computacionalmente:** Envolve muitos cÃ¡lculos de probabilidades e estimaÃ§Ã£o de parÃ¢metros.

### ğŸ¯ Principais AplicaÃ§Ãµes
* **Reconhecimento de PadrÃµes**
* **Reconhecimento de Fala**
* **AnÃ¡lise de Clientes**

<br>

## ğŸŒ³ Hierarchical Clustering

### âš™ï¸ Principais CaracterÃ­sticas
O **Hierarchical Clustering** Ã© um algoritmo de aprendizado que agrupa dados formando uma estrutura em Ã¡rvore (dendrograma), revelando relaÃ§Ãµes hierÃ¡rquicas entre os elementos. Essa tÃ©cnica permite identificar padrÃµes e conexÃµes em conjuntos de dados, sendo Ãºtil para anÃ¡lises exploratÃ³rias e visualizaÃ§Ã£o de relaÃ§Ãµes entre clusters.

### âœ… Vantagens
* **VisualizaÃ§Ã£o com Dendrograma:** FÃ¡cil de entender a estrutura dos dados.
  
* **NÃ£o precisa definir o NÃºmero de Clusters:** O nÃºmero ideal pode ser escolhido apÃ³s analisar o dendrograma.

* **Detecta Subgrupos Naturais:** Ãštil para dados com mÃºltiplas camadas de agrupamento.

### âŒ Desvantagens
* **Pouco EscalÃ¡vel:** Alto custo computacional para grandes conjuntos de dados.

* **NÃ£o se ajusta dinamicamente:** NÃ£o tem flexibilidade para corrigir junÃ§Ãµes ou divisÃµes anteriores.

* **SensÃ­vel a RuÃ­dos e Outliers:** NÃ£o possui mecanismos robustos para lidar com anomalias ao construir a hierarquia.

### ğŸ¯ Principais AplicaÃ§Ãµes
* **ReduÃ§Ã£o de Dimensionalidade e PrÃ©-processamento**
* **Agrupamento de Documentos**
* **Taxonomia BiolÃ³gica**

<br>

## ğŸ“ K-Means

### âš™ï¸ Principais CaracterÃ­sticas
O **K-Means** Ã© um algoritmo de agrupamento que divide os dados em K grupos com base na similaridade, sendo essa similaridade geralmente medida pela distÃ¢ncia euclidiana. Cada grupo Ã© representado por um **Centroide**, que corresponde Ã  mÃ©dia dos pontos pertencentes ao respectivo grupo. O algoritmo ajusta iterativamente os centroides atÃ© que eles se estabilizem, buscando minimizar a distÃ¢ncia entre os pontos e seus centroides.

### âœ… Vantagens
* **Simples de Implementar:** O modelo Ã© simples e pode ser facilmente descrito com parÃ¢metros matemÃ¡ticos.

* **EscalÃ¡vel para grandes conjuntos de dados:** Especialmente quando o nÃºmero de *clusters* Ã© pequeno.

* **FÃ¡cil de Interpretar:** Cada ponto Ã© atribuÃ­do ao grupo cujo centrÃ³ide estÃ¡ mais prÃ³ximo.

* **RÃ¡pido para dados de alta dimensionalidade:** Utiliza cÃ¡lculos vetoriais simples.

### âŒ Desvantagens
* **O nÃºmero de *clusters* K deve ser definido previamente:** Ã‰ necessÃ¡rio determinar o valor de K antes da execuÃ§Ã£o do algoritmo, o que pode ser desafiador sem conhecimento prÃ©vio sobre a estrutura dos dados.

* **SensÃ­vel a Outliers:** Pontos muito distantes podem distorcer a posiÃ§Ã£o dos centroides.

* **Pode convergir para mÃ­nimos locais:** A escolha aleatÃ³ria dos centroides iniciais pode encontrar soluÃ§Ãµes que nÃ£o sÃ£o ideais.

### ğŸ¯ Principais AplicaÃ§Ãµes
* **Agrupamento de Produtos**
* **DetecÃ§Ã£o de Anomalias**
* **AnÃ¡lise Geoespacial**

<br>

## ğŸ“‰ PCA (Principal Component Analysis)

### âš™ï¸ Principais CaracterÃ­sticas
O **PCA** Ã© um algoritmo de **reduÃ§Ã£o de dimensionalidade**. Ele transforma o conjunto de dados original em um novo sistema de coordenadas, mantendo a maior variabilidade dos dados com o menor nÃºmero de variÃ¡veis possÃ­vel (componentes principais).

### âœ… Vantagens
* **ReduÃ§Ã£o de RuÃ­do:** Ao eliminar componentes menos relevantes, melhora o desempenho de outros modelos.

* **Facilidade de VisualizaÃ§Ã£o:** Facilita a anÃ¡lise visual de datasets multidimensionais.

* **Melhora o Desempenho Computacional:** Reduz a dimensionalidade mantendo a maior parte da informaÃ§Ã£o.

### âŒ Desvantagens
* **Perda de Interpretabilidade:** As novas variÃ¡veis (componentes principais) nÃ£o tÃªm significado direto.

* **Assume Linearidade:** NÃ£o captura bem relaÃ§Ãµes nÃ£o lineares.

* **Pode eliminar InformaÃ§Ãµes Importantes:** Se os componentes principais nÃ£o representarem bem a variabilidade dos dados.

### ğŸ¯ Principais AplicaÃ§Ãµes
* **PrÃ©-processamento de Dados para Machine Learning**
* **VisualizaÃ§Ã£o de Dados Complexos**
* **CompressÃ£o de Dados**

<br>

## ğŸŒ€ Kernel PCA (Principal Component Analysis com NÃºcleo)

### âš™ï¸ Principais CaracterÃ­sticas
O **Kernel PCA** Ã© uma extensÃ£o nÃ£o linear do PCA tradicional que utiliza funÃ§Ãµes kernel para mapear os dados originais para um espaÃ§o de alta dimensÃ£o, onde Ã© possÃ­vel realizar a anÃ¡lise de componentes principais de forma linear. Isso permite capturar estruturas nÃ£o lineares dos dados que o PCA clÃ¡ssico nÃ£o consegue detectar.

### âœ… Vantagens
* **Capta RelaÃ§Ãµes NÃ£o Lineares:** Permite identificar componentes principais em dados com estrutura nÃ£o linear.
  
* **FlexÃ­vel:** Pode usar diferentes tipos de kernels (RBF, polinomial, sigmoidal, etc.) para adaptar-se ao formato dos dados.

* **Melhora a ReduÃ§Ã£o de Dimensionalidade:** Pode melhorar a representaÃ§Ã£o dos dados em espaÃ§os menores mantendo a estrutura relevante.

### âŒ Desvantagens
* **Mais Complexo Computacionalmente:** Para grandes conjuntos de dados, o cÃ¡lculo da matriz kernel pode ser caro em tempo e memÃ³ria.

* **SensÃ­vel Ã  Escolha do Kernel:** O desempenho depende fortemente da escolha do kernel e seus parÃ¢metros.

* **Menos InterpretÃ¡vel:** Os componentes principais resultantes em espaÃ§os kernel sÃ£o menos intuitivos do que no PCA clÃ¡ssico.

### ğŸ¯ Principais AplicaÃ§Ãµes
* **Reconhecimento de PadrÃµes NÃ£o Lineares**
* **ReduÃ§Ã£o de Dimensionalidade para Dados Complexos**
* **PrÃ©-processamento de Dados para Machine Learning**

<br>

## ğŸ—ºï¸ t-SNE (t-Distributed Stochastic Neighbor Embedding)

### âš™ï¸ Principais CaracterÃ­sticas
O **t-SNE** Ã© um algoritmo de reduÃ§Ã£o de dimensionalidade nÃ£o linear, especialmente eficaz para visualizaÃ§Ã£o de dados em 2D ou 3D. Ele transforma as distÃ¢ncias entre pontos em probabilidades de similaridade e busca preservar as relaÃ§Ãµes locais.

### âœ… Vantagens
* **Bom para VisualizaÃ§Ã£o:** Ã‰ eficaz para representar dados complexos em duas ou trÃªs dimensÃµes.

* **Captura RelaÃ§Ãµes Locais:** MantÃ©m a proximidade entre os pontos similares, revelando padrÃµes e agrupamentos.

* **Revela Estrutura dos Dados:** Muitas vezes revela clusters que nÃ£o seriam evidentes com mÃ©todos lineares.

### âŒ Desvantagens
* **NÃ£o EscalÃ¡vel para Grandes Conjuntos de Dados:** A complexidade computacional cresce rapidamente com o tamanho dos dados.

* **SensÃ­vel a ParÃ¢metros:** O resultado depende da escolha certa de parÃ¢metros como *perplexity* e *learning rate*.

* **NÃ£o Ã© DeterminÃ­stico:** ExecuÃ§Ãµes repetidas podem gerar resultados diferentes.

### ğŸ¯ Principais AplicaÃ§Ãµes
* **VisualizaÃ§Ã£o de Dados de Alta Dimensionalidade**
* **AnÃ¡lise ExploratÃ³ria de Agrupamentos**
* **ReduÃ§Ã£o de Dimensionalidade para Dados de Imagem**

<br>

## ğŸ”— ReferÃªncias

### ğŸ“š Conceitos Gerais de Aprendizado NÃ£o Supervisionado
* [Oracle - Aprendizado NÃ£o Supervisionado](https://www.oracle.com/br/artificial-intelligence/machine-learning/unsupervised-learning/)
* [Deep Learning Book](https://www.deeplearningbook.com.br/)
* [Kaggle â€“ Iris Dataset](https://www.kaggle.com/datasets/uciml/iris)

---

### ğŸ§² Affinity Propagation
* [Scikit-Learn â€“ Affinity Propagation](https://scikit-learn.org/stable/modules/clustering.html#affinity-propagation)
* [Wikipedia â€“ Affinity Propagation](https://en.wikipedia.org/wiki/Affinity_propagation)

---

### ğŸ“¦ DBSCAN
* [Scikit-Learn â€“ DBSCAN](https://scikit-learn.org/stable/modules/clustering.html#dbscan)
* [Medium â€“ Entendendo DBSCAN](https://gabriellm.medium.com/entendendo-dbscan-770f680d9160)

---

### ğŸ“Š Gaussian Mixture Model
* [Scikit-Learn â€“ Gaussian Mixture](https://scikit-learn.org/stable/modules/mixture.html#gaussian-mixture)
* [Built In â€“ Gaussian Mixture Model](https://builtin.com/articles/gaussian-mixture-model)

---

### ğŸŒ³ Hierarchical Clustering
* [Scikit-Learn â€“ Hierarchical Clustering](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering)
* [IBM â€“ Hierarchical Clustering](https://www.ibm.com/think/topics/hierarchical-clustering)

---

### ğŸ“ K-Means
* [Scikit-Learn â€“ K-Means](https://scikit-learn.org/stable/modules/clustering.html#k-means)
* [IBM â€“ K-Means Clustering](https://www.ibm.com/br-pt/think/topics/k-means-clustering)

---

### ğŸ“‰ PCA (Principal Component Analysis)
* [Scikit-Learn â€“ Principal Component Analysis (PCA)](https://scikit-learn.org/stable/modules/decomposition.html#principal-component-analysis-pca)
* [IBM â€“ Principal Component Analysis](https://www.ibm.com/think/topics/principal-component-analysis)

---

### ğŸŒ€ Kernel PCA (Principal Component Analysis com NÃºcleo)
* [Scikit-Learn â€“ Kernel Principal Component Analysis (KPCA)](https://scikit-learn.org/stable/modules/decomposition.html#kernel-principal-component-analysis-kpca)
* [Baeldung â€“ Kernel Principal Component Analysis (KPCA)](https://www.baeldung.com/cs/kernel-principal-component-analysis)  

---

### ğŸ—ºï¸ t-SNE (t-Distributed Stochastic Neighbor Embedding)
* [Scikit-Learn â€“ t-Distributed Stochastic Neighbor Embedding (t-SNE)](https://scikit-learn.org/stable/modules/manifold.html#t-distributed-stochastic-neighbor-embedding-t-sne)  
* [DataCamp â€“ IntroduÃ§Ã£o ao t-SNE](https://www.datacamp.com/pt/tutorial/introduction-t-sne)  
* [Wikipedia â€“ t-distributed Stochastic Neighbor Embedding](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)
